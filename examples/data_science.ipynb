{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Vu0JiMB6AA6L",
        "q0xCcXSZkov-",
        "F4WKALKZVOhB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "y2okmtchXi2L"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/Totemi1324/neural-networks-demo/main/datasets/melb_data.csv';\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "def preprocess_dataset(raw):\n",
        "  y = raw.Price\n",
        "  features = raw.drop(['Price'], axis=1)\n",
        "  X = features.select_dtypes(exclude=['object'])\n",
        "  return X, y\n",
        "\n",
        "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
        "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_valid)\n",
        "    print(f\"MAE: {mean_absolute_error(y_valid, preds)}\")\n",
        "\n",
        "X_display, y_display = preprocess_dataset(data)\n",
        "display(X_display)"
      ],
      "metadata": {
        "id": "Js05U0DMXnP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Excercise 1: Handling NaN values"
      ],
      "metadata": {
        "id": "Vu0JiMB6AA6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1: Drop all columns with NaN values\n",
        "\n",
        "![Drop NaN columns](https://raw.githubusercontent.com/Totemi1324/neural-networks-demo/main/assets/drop_nan_columns.PNG)"
      ],
      "metadata": {
        "id": "X723muTrAjUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data.dropna(axis=1)\n",
        "X_processed, y = preprocess_dataset(data1)\n",
        "\n",
        "display(X_processed)"
      ],
      "metadata": {
        "id": "9n3WuAjPAAPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2: Drop all rows with NaN values"
      ],
      "metadata": {
        "id": "-0wOjhONDaTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = data.dropna()\n",
        "X_processed, y = preprocess_dataset(data2)\n",
        "\n",
        "display(X_processed)"
      ],
      "metadata": {
        "id": "xiBeq4HVDZ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 3: Imputation; fill NaN fields with the mean of their column\n",
        "\n",
        "![Drop NaN columns](https://raw.githubusercontent.com/Totemi1324/neural-networks-demo/main/assets/drop_nan_columns.PNG)"
      ],
      "metadata": {
        "id": "TmxNKPjgM4Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocess_dataset(data)\n",
        "\n",
        "imputer = SimpleImputer()\n",
        "X_processed = pd.DataFrame(imputer.fit_transform(X))\n",
        "X_processed.columns = X.columns\n",
        "\n",
        "display(X_processed)"
      ],
      "metadata": {
        "id": "AYyIZHEJM4_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 4: Extend imputation by an additional column\n",
        "\n",
        "![Drop NaN columns](https://raw.githubusercontent.com/Totemi1324/neural-networks-demo/main/assets/imputation_extended.PNG)"
      ],
      "metadata": {
        "id": "Yx6RtKP3PujP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocess_dataset(data)\n",
        "\n",
        "columns_with_missing = [column for column in X.columns if X[column].isnull().any()]\n",
        "\n",
        "for column in columns_with_missing:\n",
        "    X[column + '_was_missing'] = X[column].isnull()\n",
        "\n",
        "imputer = SimpleImputer()\n",
        "X_processed = pd.DataFrame(imputer.fit_transform(X))\n",
        "X_processed.columns = X.columns\n",
        "\n",
        "display(X_processed)"
      ],
      "metadata": {
        "id": "yV4U5cbxPuYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose one option of the above. To evaluate, execute the code block below:"
      ],
      "metadata": {
        "id": "4mtAP6G_BAmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_processed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "score_dataset(X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "train_combined = X_train.copy()\n",
        "train_combined[\"Price\"] = y_train\n",
        "\n",
        "train_combined.boxplot(by=\"Rooms\", column=[\"Price\"])"
      ],
      "metadata": {
        "id": "L3137i43BJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Excercise 2: Changing train/test split"
      ],
      "metadata": {
        "id": "q0xCcXSZkov-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the fractions for the train and test sets:"
      ],
      "metadata": {
        "id": "EM80GXnZkotR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_percentage = 0.85\n",
        "test_percentage = 0.15"
      ],
      "metadata": {
        "id": "a4ijbh3gkofO"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate, execute the code block below:"
      ],
      "metadata": {
        "id": "o0Ff_1rclegP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_processed, y, train_size=train_percentage, test_size=test_percentage, random_state=0)\n",
        "\n",
        "score_dataset(X_train, X_valid, y_train, y_valid)"
      ],
      "metadata": {
        "id": "TJV6SC8ZleL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Excercise 3: Normalizing numerical data"
      ],
      "metadata": {
        "id": "F4WKALKZVOhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn on/off normalization:"
      ],
      "metadata": {
        "id": "aLL9iVrocOaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "should_normalize = False"
      ],
      "metadata": {
        "id": "1JjWgpXLcORr"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate, execute the code block below:"
      ],
      "metadata": {
        "id": "MP6Kp4u2cssh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if should_normalize:\n",
        "  X_train_norm = X_train.div(X_train.sum(axis=1), axis=0)\n",
        "  X_valid_norm = X_valid.div(X_valid.sum(axis=1), axis=0)\n",
        "else:\n",
        "  X_train_norm = X_train.copy()\n",
        "  X_valid_norm = X_valid.copy()\n",
        "\n",
        "display(X_train_norm)\n",
        "\n",
        "score_dataset(X_train_norm, X_valid_norm, y_train, y_valid)\n",
        "\n",
        "sns.pairplot(X_train_norm[['Rooms', 'BuildingArea', 'PropertyCount']], diag_kind='kde')"
      ],
      "metadata": {
        "id": "Ww3z2PFQcoJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful places to get data from (for practice or real-world applications):\n",
        "\n",
        "- [Kaggle](https://www.kaggle.com/): Largest repository of publicly available datasets (over 50 000) for data science applications\n",
        "- [Data.gov](https://data.gov/): US government's open data repository. Topics: agriculture and climate, energy, marine transportation, and more.\n",
        "- [NASA Open Data Portal](https://data.nasa.gov/): Catalog of publicly available NASA datasets. Topics: national aeronautics and space data, physical oceanography, ocean biology data, earth resources observations, social-economic data, and more.\n",
        "- [Earthdata by NASA](https://earthdata.nasa.gov/): NASA dataset catalog partition for earth-related collections. Topics: atmosphere, land, ocean, cryosphere, and more.\n",
        "- [NASDAQ Data Link](https://data.nasdaq.com/): Data source from the world's largest technology fund. Topics: finance and economics, stock prices, trading activity, interest rate dynamics, and more.\n",
        "\n",
        "There are many projects that start by first scouting for interesting datasets, so feel free to browse and let these inspire you!"
      ],
      "metadata": {
        "id": "7etqwWksZQiH"
      }
    }
  ]
}